Tokenizer:
  max_length: 64
InputLayer:
  vocab_size: 134
  emb_dim: 12
  max_length: 64
Attention:
  emb_dim: 12
  dropout: 0.1
MultiHeadAttention:
  emb_dim: 12
  head: 4
  hidden_dim: 40 # emb_dim * 4
  dropout: 0.1
OutputLayer:
  vocab_size: 134
  emb_dim: 12
GPT:
  vocab_size: 134
  emb_dim: 12
  max_length: 64
  num_blocks: 8
  head: 4
  hidden_dim: 40
  dropout: 0.1